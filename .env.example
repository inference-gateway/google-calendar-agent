# Application Configuration
APP_ENVIRONMENT=dev
APP_DEBUG=false
APP_DEMO_MODE=false
APP_MAX_REQUEST_SIZE=1048576
APP_REQUEST_TIMEOUT=30s

# Google Calendar Configuration
GOOGLE_CALENDAR_ID=primary
# Option 1: Provide JSON credentials directly (will be written to a file)
GOOGLE_CALENDAR_SA_JSON=
# Option 2: Provide path to existing credentials file (if GOOGLE_CALENDAR_SA_JSON is empty)
# If GOOGLE_CALENDAR_SA_JSON is provided and GOOGLE_APPLICATION_CREDENTIALS is empty, 
# defaults to /tmp/google-credentials.json
GOOGLE_APPLICATION_CREDENTIALS=
GOOGLE_CALENDAR_READ_ONLY=false
GOOGLE_CALENDAR_TIMEZONE=Europe/Berlin

# Server Configuration
SERVER_PORT=8080
SERVER_HOST=0.0.0.0
SERVER_GIN_MODE=release
SERVER_ENABLE_TLS=false
SERVER_READ_TIMEOUT=10s
SERVER_WRITE_TIMEOUT=10s
SERVER_IDLE_TIMEOUT=60s

# Logging Configuration
LOG_LEVEL=info
LOG_FORMAT=json
LOG_OUTPUT=stdout
LOG_ENABLE_CALLER=true
LOG_ENABLE_STACKTRACE=true

# TLS Configuration (when SERVER_ENABLE_TLS=true)
TLS_CERT_PATH=
TLS_KEY_PATH=
TLS_MIN_VERSION=1.2
TLS_CIPHER_SUITES=

# LLM Configuration (Natural Language Processing)
# Enable or disable LLM functionality
LLM_ENABLED=true

# Inference Gateway URL or OpenAI-compatible API endpoint
LLM_GATEWAY_URL=http://localhost:8080/v1

# LLM Provider - supported providers via Inference Gateway:
# - openai: OpenAI GPT models
# - anthropic: Anthropic Claude models  
# - groq: Groq models (fast inference)
# - ollama: Local Ollama models
# - deepseek: DeepSeek models
# - cohere: Cohere models
# - cloudflare: Cloudflare Workers AI models
LLM_PROVIDER=groq

# Model to use (examples for different providers):
# OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
# Anthropic: claude-3-opus, claude-3-sonnet, claude-3-haiku
# Groq: deepseek-r1-distill-llama-70b, qwen-2.5-coder-32b, llama-3.1-70b-versatile
# Ollama: llama3.2, qwen2.5, codellama
# DeepSeek: deepseek-chat
# Cohere: command-r-plus, command-r
# Cloudflare: @cf/meta/llama-3.1-8b-instruct
LLM_MODEL=deepseek-chat

# Request timeout for LLM calls
LLM_TIMEOUT=30s

# Maximum tokens to generate
LLM_MAX_TOKENS=2048

# Temperature for response generation (0.0 = deterministic, 2.0 = very creative)
LLM_TEMPERATURE=0.7
